{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This script:\n",
    "1. Scrapes the Hacker News website for the latest headlines and links.\n",
    "2. Inserts the headlines, links, and link content into opensearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_all_things(url):\n",
    "    # Fetch the list of 'things' from hackernews\n",
    "    response = requests.get(url)\n",
    "    content = response.content\n",
    "\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    things = soup.find_all('tr', class_='athing')\n",
    "    next_page = soup.find('a', class_='morelink')\n",
    "\n",
    "    return things, next_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPageText(url):\n",
    "    try:\n",
    "        html_content = requests.get(url).content\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        [s.extract() for s in soup(['style', 'script', '[document]', 'head', 'title'])]\n",
    "        text = soup.getText(separator='\\n')\n",
    "        return '\\n'.join(line.strip() for line in text.split('\\n') if line.strip())\n",
    "    except TimeoutError:\n",
    "        print(f'Warning: Timeout error requesting content from {url}')\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class HackerNewsPage():\n",
    "    # hackernews index pages have 30 listings each\n",
    "    # the thing object passed to init is the \n",
    "    # `titleline > a` is the link to the thing\n",
    "    # `subline:last-child` is the comments link\n",
    "    def __init__(self, thing):\n",
    "        link_element_obj = thing.select('.titleline > a')[0]\n",
    "        self.headline = link_element_obj.text\n",
    "        self.thing_url = link_element_obj['href']\n",
    "        base_url = 'https://news.ycombinator.com/'\n",
    "\n",
    "        if 'http' not in self.thing_url:\n",
    "            self.thing_url=f'{base_url}{self.thing_url}'\n",
    "        print(f'using {self.thing_url}')\n",
    "        self.thing_content = getPageText(self.thing_url)\n",
    "\n",
    "        # get comments, which are in next parent sibling\n",
    "        try:\n",
    "            comments = thing.next_sibling.select('.subline > a')[-1]\n",
    "            self.comments_url = f'{base_url}{comments[\"href\"]}'\n",
    "            self.comments_content = getPageText(self.comments_url)\n",
    "        except IndexError:\n",
    "            # no comments, so no link to comments page\n",
    "            self.comments_url = ''\n",
    "            self.comments_content = ''\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f'Headline: {self.headline}\\nThing URL: {self.thing_url}\\nThing contents: {self.thing_content[:20]}\\nComments URL: {self.comments_url}\\nComments: {self.comments_content[:20]}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each page has 30 links denoted by html class _athing\n",
    "# each thing has 2 pages to gather: the thing linked to and the comments\n",
    "# example 3 pages = 3*30*2 = 180. So 180/2=90 requests to the site, 180/2=90 requests to other sites, and 180 entries to opensearch\n",
    "NUM_PAGES = 1\n",
    "base_url = 'https://news.ycombinator.com/'\n",
    "all_things = [] # list of HackerNewsPage objects\n",
    "current_url=base_url\n",
    "for i in range(NUM_PAGES):\n",
    "    print(f'Getting hackernews page {i+1}')\n",
    "    # step 1 find the tbody\n",
    "\n",
    "    things, next_page = get_all_things(current_url)\n",
    "    print(f'Found {len(things)} from page {i+1}')\n",
    "    all_things.extend([HackerNewsPage(thing) for thing in tqdm(things)])\n",
    "    print(f'Got {len(things)} links on page {i+1}')\n",
    "    \n",
    "    if not next_page or not (current_url := f'{base_url}{next_page[\"href\"]}') or current_url == base_url:\n",
    "        print(f'No more pages to scrape.')\n",
    "        break\n",
    "\n",
    "print(f'Total links: {len(all_things)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for thing in all_things:\n",
    "    print(thing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenSearch:\n",
    "    # rest API documentation: https://opensearch.org/docs/latest/api-reference/\n",
    "    def __init__(self, host, port):\n",
    "        self.host = host\n",
    "        self.port = port\n",
    "        self.url = f'http://{self.host}:{self.port}'\n",
    "        self.headers = {'Content-Type': 'application/json'}\n",
    "        try:\n",
    "            self.session = requests.Session()\n",
    "        except ConnectionError:\n",
    "            print('Connection to OpenSearch failed.')\n",
    "\n",
    "    def create_index(self, index_name, mapping):\n",
    "        url = f'{self.url}/{index_name}'\n",
    "        response = requests.put(url, headers=self.headers, data=json.dumps(mapping))\n",
    "        return response.json()\n",
    "\n",
    "    def insert_document(self, index_name, document):\n",
    "        url = f'{self.url}/{index_name}/_doc'\n",
    "        response = requests.post(url, headers=self.headers, data=json.dumps(document))\n",
    "        print(f'Sending document object:\\n{json.dumps(document)}')\n",
    "\n",
    "        return response.json()\n",
    "\n",
    "    def bulk_insert(self, index_name, documents):\n",
    "        # bulk is recommended: https://opensearch.org/docs/latest/api-reference/document-apis/bulk/\n",
    "        url = f'{self.url}/{index_name}/_bulk'\n",
    "        data = '\\n'.join([json.dumps({\"create\":{}}) + '\\n' + json.dumps(doc) for doc in documents]) + '\\n'\n",
    "        print(f'Sending bulk document object:\\n{data}')\n",
    "        response = requests.post(url, headers=self.headers, data=data)\n",
    "        return response.json()\n",
    "\n",
    "    def delete_index(self, index_name):\n",
    "        url = f'{self.url}/{index_name}'\n",
    "        response = requests.delete(url, headers=self.headers)\n",
    "        return response.json()\n",
    "\n",
    "    def delete_all(self, index_name):\n",
    "        query = {\n",
    "            \"query\": {\n",
    "                \"match_all\": {}\n",
    "            }\n",
    "        }\n",
    "        url = f'{self.url}/{index_name}/_delete_by_query'\n",
    "        response = requests.post(url, headers=self.headers, data=json.dumps(query))\n",
    "        return response.json()\n",
    "\n",
    "    def delete_by_query(self, index_name, query):\n",
    "        url = f'{self.url}/{index_name}/_delete_by_query'\n",
    "        response = requests.post(url, headers=self.headers, data=json.dumps(query))\n",
    "        return response.json()\n",
    "\n",
    "    def count(self, index_name):\n",
    "        url = f'{self.url}/{index_name}/_count'\n",
    "        response = requests.get(url, headers=self.headers)\n",
    "        return response.json()\n",
    "\n",
    "    def search(self, index_name, query):\n",
    "        url = f'{self.url}/{index_name}/_search'\n",
    "        response = requests.get(url, headers=self.headers, data=json.dumps(query))\n",
    "        return response.json()\n",
    "        \n",
    "    def analyze(self, index_name, text_array):\n",
    "        url = f'{self.url}/{index_name}/_analyze'\n",
    "        query = {\n",
    "            \"analyzer\" : \"standard\",\n",
    "            \"text\" : text_array\n",
    "        }\n",
    "        response = requests.get(url, headers=self.headers, data=json.dumps(query))\n",
    "        return response.json()\n",
    "\n",
    "opensearch_instance = OpenSearch('localhost', 9200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hackernews_mapping = {\n",
    "    \"settings\": {\n",
    "    \"index\": {\n",
    "      \"number_of_shards\": 1,\n",
    "      \"number_of_replicas\": 0\n",
    "    }\n",
    "  },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"title\": {\"type\": \"text\"},\n",
    "            \"url\": {\"type\": \"keyword\"},\n",
    "            \"comments\": {\"type\": \"text\"},\n",
    "            \"article\": {\"type\": \"text\"}\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create index\n",
    "opensearch_instance.create_index('hackernews', hackernews_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert a single documentthing = all_things[0]\n",
    "document = {\n",
    "    \"title\": thing.headline,\n",
    "    \"url\": thing.thing_url,\n",
    "    \"comments\": thing.comments_content,\n",
    "    \"article\": thing.thing_content\n",
    "}\n",
    "\n",
    "\n",
    "opensearch_instance.insert_document('hackernews', document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for thing in all_things:\n",
    "    document = {\n",
    "        \"title\": thing.headline,\n",
    "        \"url\": thing.thing_url,\n",
    "        \"comments\": thing.comments_content,\n",
    "        \"article\": thing.thing_content\n",
    "    }\n",
    "    documents.append(document)\n",
    "opensearch_instance.bulk_insert('hackernews', documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opensearch_instance.delete_index('hackernews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opensearch_instance.count('hackernews')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Time!\n",
    "#### Text Fields\n",
    "- Full-text Search:\n",
    "  - Use Match and Match Phrase queries for natural language search.\n",
    "    - Example: Searching for articles or comments with common language terms.\n",
    "- Multi-field Search:\n",
    "  - Multi-Match query to search across several text fields.\n",
    "    - Useful when the search term may appear in multiple text areas (e.g., title, article).\n",
    "- Wildcard and Regex:\n",
    "  - For pattern-based searches, where exact term is unknown.\n",
    "  - Useful for partial matches in larger text fields.\n",
    "#### Keyword Fields:\n",
    "- Exact Match:\n",
    "    - Use Term queries for exact matching.\n",
    "    - Ideal for URLs or specific identifiers.\n",
    "- Aggregations and Sorting:\n",
    "  - Aggregate data based on exact keyword values.\n",
    "    - Sort query results using keyword fields for consistent ordering.\n",
    "- Wildcard with Caution:\n",
    "  - Can be used, but less efficient than with text fields.\n",
    "    - Suitable for keywords with predictable patterns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# match query on text field\n",
    "query = {\n",
    "  \"query\": {\n",
    "    \"match\": {\n",
    "      \"title\": \"Work Microchips\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "opensearch_instance.search('hackernews', query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match phrase query on text field\n",
    "query = {\n",
    "  \"query\": {\n",
    "    \"match_phrase\": {\n",
    "      \"article\": \"Microchips Work\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "opensearch_instance.search('hackernews', query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi-match query on multiple text fields\n",
    "query = {\n",
    "  \"query\": {\n",
    "    \"multi_match\": {\n",
    "      \"query\": \"How\",\n",
    "      \"fields\": [\"title\", \"article\"]\n",
    "    }\n",
    "  }\n",
    "}\n",
    "opensearch_instance.search('hackernews', query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# term query on keyword field\n",
    "query = {\n",
    "  \"query\": {\n",
    "    \"term\": {\n",
    "      \"url\": \"https://publicdomainreview.org/collection/wierix-flemish-proverbs/\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "opensearch_instance.search('hackernews', query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregation on keyword field\n",
    "query = {\n",
    "  \"size\": 0,\n",
    "  \"aggs\": {\n",
    "    \"unique_urls\": {\n",
    "      \"terms\": {\n",
    "        \"field\": \"url\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "opensearch_instance.search('hackernews', query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wildcard query on text field\n",
    "query = {\n",
    "  \"query\": {\n",
    "    \"wildcard\": {\n",
    "      \"url\": \"*github*\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "opensearch_instance.search('hackernews', query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wildcard query on text field\n",
    "text_array=['hello', 'first array element', 'second array element']\n",
    "\n",
    "opensearch_instance.analyze('hackernews', text_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
